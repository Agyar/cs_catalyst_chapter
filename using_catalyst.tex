\section{Using Catalyst}
\label{sec:catalyst}

Catalyst is the coprocessing library of ParaView. 
It has been designed to be tightly coupled with simulation codes to perform in situ analysis at run time. 
Catalyst leverages the Visualization Toolkit (VTK) for scalable data analysis and visualization. Furthermore, it can be coupled with the ParaView In Situ Analysis framework to perform run-time visualization of data extracts and steering of the data analysis pipeline.

Catalyst provides two sets of tools: one for simulation users and one for simulation developers.
For simulation users, it is possible to create a coprocessing pipeline using two different methods. 
The first method does not require any knowledge of ParaView and relies on pre-generated scripts. These predefined scripts can be written in C++ or Python and are, usually, expected to run without any configuration options. 
The second method uses the ParaView interface to generate a coprocessing script from scratch and intuitively adjust its parameters as needed. This method is similar to using ParaView interactively to setup desired post-processing pipelines. The goal of these pipelines is to extract post-processed information during the simulation run. Ideally one should start with a representative dataset from the simulation. It is also possible to modify directly the generated Python's scripts which have been previously created using ParaView. However, this would require a knowledge of the ParaView Python API.

For simulation developers, Catalyst provides the tools to create an adaptor between the simulation code and the visualization pipeline.
The adaptor binds the simulation code and Catalyst so that both the functions of the simulation code and the general-purpose application programming interface (API) of Catalyst can be accessed. As Catalyst itself is independent of the simulation code, only the adaptor has to be developed by the designers of the solver. This flexibility is critical in order to successfully integrate external code into complex simulations usually running with different languages and packages. Catalyst is also easily extensible so that users can deploy new analysis and visualization techniques to existing coprocessing installations. Catalyst provides all the communication and synchronization routine and the pipeline mechanics necessary for coprocessing. Catalyst also provides powerful data processing capabilities through VTK filters as well as many writers and support for compositing and rendering.

Catalyst has also been developed with features to address limitations that come with pre-configuring a pipeline, but there may still be some unexpected data in the arbitrary simulation. To address these situations, the pipeline must be adjusted interactively.
The Catalyst library can leverage ParaView's client server mechanism to allow an interactive ParaView client to connect to a server running inside an in situ pipeline. ParaView can then read from a Catalyst data source like it reads from a file. This offers to construct/modify a pipeline interactively in the ParaView client via this live data source. Additionally, by enabling the client-server architecture in the Catalyst library, some or all of the data analysis and visualization pipeline can offload, if desired, to a separate machine, e.g., a smaller visualization cluster with specialized graphics hardware.

~\
\includegraphics[scale=0.47]{pictures/CatalystFullWorkFlow.eps}
\captionof{figure}{Overall Catalyst workflow.}
\label{fig:catalyst}
\vspace{+0.04in}
~\

\subsection{\CS with Catalyst}

Using Catalyst with \CS is quite straightforward, and fits
quite naturally in the existing architecture, which we describe first.

\subsubsection{Pre-existing post-processing architecture}

To make it possible to work with large meshes and potientally long-running
simulations of unsteady flows, multiple postprocessing-related features
are available in the code. The user may specify 3D visualisation
output of selected main and auxiliary variables, time-plot data output
for selected cell values (``probes''), and 1-D profiles, as well as
choose to compute and log or output additional values by programming
a user function called at the end of each time step.
In this way, low-volume and global variables may be output
at each time step, and higher-volume output may be output with lower
frequency.

Mesh-based postprocessing output uses the concepts of \emph{meshes}
and \emph{writers}, as shown in the following figure:

~\
\includegraphics[scale=0.3]{pictures/cs_post.eps}
\captionof{figure}{\CS postprocessing output subsystem.}
\label{fig:cs_post}
\vspace{+0.04in}
~\

A postprocessing mesh may be any sub-mesh of the computation mesh (or even
a separate auxiliary mesh in some cases), which may optionnaly
be redefined over time steps, while a writer is a combination of the
selection of an output format (such as EnSight gold, MED, or CGNS),
an output frequency, sub-directory and file name prefix,
and additional metadata specifying for example whether associated meshes
will be fixed or time varying (if the selected format supports it).
To each postprocessing \emph{mesh} may be associated a list of
\emph{writers} among those defined (an empty list meaning no output).
By default, the global volume mesh and boundary \emph{meshes} are
associated to a main \emph{writer}, whose default settings include the
EnSight gold format and output at the last time step only.
The user may modify those settings and associations, as well as
define additional \emph{meshes} and \emph{writers}, both using
the GUI and user functions (useful for more advanced cases
and finer control).
Using this system, output at different mesh formats, with different
frequencies, and possibly different variables may be generated
simultaneously, allowing fine tuning depending on the indended
usage.

\CS suggests a clear separation of tasks best handed by the code
or those handled by external tools. For example, although computing
a gradient may be handled directly by visualization tools, it is
best done in the user-defined output of the code itself, to ensure
it is done in a manner consistent with the discretization, and
exploiting the knowledge of prescribed boundary conditions.
On the other hand, postprocessing \emph{meshes} are defined as
subsets of the computational mesh, but generation of clips and slices
are not handled by the code, as it is expected visualization tools
may handled those geometric operations. Both may be combined: for example,
a volume sub-mesh involving cells around a clip plane may be output
by \CS, reducing the data that needs to be handled by a visualization
tool to produce that clip.

\subsubsection{Adaptor implementation}
When embedding Catalyst there is always a non-zero cost in terms of time, memory and the number of processors. Naively, one could simply
address these issues by simply requesting a greater number of processors, but in most cases this simply is
not possible or practical. Therefore, great care must be taken in the construction of the adaptors.
If memory is limited, the adaptor either uses complicated pointer manipulation or uses a smaller region
of memory. If memory is not limited, then deep copy of the simulation data structures into a VTK
data object doubles the resident memory, but also creates a CPU cost involved with copying data.
The overriding overhead issue in embedding ParaView Catalyst in a simulation code is the memory
management in an adaptor translating data structures in the running simulation.

In order to coprocess the simulation data, Catalyst must be provided with the
data formatted to the VTK data object structure. To accomplish this, several
solutions are possible, essentially depending on the format used for the data
of the simulation code. In the case when the format of the simulation code is
similar to VTK  and, moreover, the simulation data can be shared at any time,
then it is possible to feed Catalyst with a direct pointer to the simulation
memory. This option is indeed preferred, when possible, as it allows to decrease the memory footprint.
Another option is to fully or partially copy the data from the
simulation into a VTK object, and then send this object to Catalyst.

As users of \CS are provided with several output formats and as the data
structure in our simulation differs from the VTK data object structure, feeding
Catalyst with a direct pointer to the simulation memory is not possible. Thus, in this configuration
data is copied from the simulation into a VTK data object. In fact, we
 allocate a vtkDoubleArray data structure to store our data for Catalyst. Furthermore, we
provide a pointer of this VTK data structure to \CS so it can transform
its simulation data and then fill the VTK data object.

The memory cost increase of our solution can be alleviated by using more
machines. The CPU cost of the copy is in a range similar to the one needed when
adapting simulation data to a specific output format. This cost is largely
affordable comparatively to the time to write data to disk when storing time
step specific outputs. 

\subsubsection{Pipeline configuration}
\label{sec:pip_conf_tools}

From the point of view of an engineer performing a fluid mechanics simulation
using \CS, the workflow of a coprocessing-simulation is 1) to define
a ParaView pipeline describing what the user wants to study and 2) to run the
simulation. Since users are already familiar with fluid mechanics
simulations, defining the pipeline for the coprocessing remains the main
sticking point. Thus this new process should be done in an efficient way and 
should not become a cumbersome bottleneck. This point is of great importance, especially in
an industrial environment like ours.

As we have explained in the previous section, the definition of a Catalyst pipeline can be achieve either 
programmatically or via the ParaView interface. In our industrial context, the former 
was considered too complicated and time consuming for the end user, especially when setting camera
parameters is needed, as no visualization feedback is provided. Therefore, the Catalyst pipeline has 
been created using the ParaView user's interface. This solution appears to be much easier as one
can simply interact with ParaView in the same way he/she uses to when
visualizing the results a posteriori. This solution is also easier to deploy with ParaView using the 
its companion coprocessing plugin. 

Indeed, using ParaView to define the coprocessing requires a representative input dataset.
One could use the available resources on a large cluster in order to setup the pipeline. However we chose to  
provide a simplified or under-sampled version of the large geometry to define the
pipeline. In fact, this strategy is possible in ParaView but some
characteristics of the initial geometry must be present in its simplified version; more importantly the name of the data fields must
remain the same, as they are part of the definition of the pipeline.

The generation of the coprocessing pipeline implies several steps.
First of all, the users start with a CAD (Computer Aided Design)
version of the geometry which is parametrized. This parametric representation
can generate meshes at different resolution levels. In our case, this is
performed inside the open-source SALOME~\cite{4291178} platform for numerical simulation. 

We then generate two different meshes, one at high resolution (up to 204M
hexahedrals in the current use cases) that will be used for the CFD
simulation and one with a lower resolution to define the pipeline (700 000
hexahedrals in our use cases). The lowest resolution mesh is fed into \CS to perform a short
simulation. This allows ParaView to obtain a representation containing not
only the geometry but also the result fields. This is the data that is then used to define the pipeline.
The different processing pipelines are presented next.
