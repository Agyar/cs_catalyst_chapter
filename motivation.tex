\section{Motivation}
\label{sec:motivation}

Most numerical simulation engineers at EDF R\&D are currently visualizing lower
temporal and spatial resolution versions of their simulations, in order to avoid
I/O issues and cumbersome visualisation procedures, when large quantities of
data are involved. We believe that other industries dealing with large
simulations are having the same problem. This is the reason why we decided to
address the subject of coprocessing and in-situ visualization. Our aim was to
provide our research-oriented engineers with an operational tool in a mid-term
basis. Thus, we have evaluated Catalyst as an industrial tool for performing
In-Situ visualization. 

First of all, it is important to better describe the scope of our industrial
visualisation solutions before in-situ processing was being tested. In table XXX
we show the results of a simple subjective experiment conducted by one of our
engineers. At the end of 2012, she meshed a simple cube at different resolutions
and then tried to visualise the results giving a subjective evaluation of how
she could work. She used SALOME (our open-source numerical simulation platform)
and an EDF R\&D standard scientific PC that contains 8 Gb of RAM. The
visualisation system of SALOME is an integration of ParaView. Table XXX presents
the results of her subjective experiment. The study clearly shows that she
started working without an immediate system response for meshes which contain
more that 10 Millions cells and for 50 Million cells the system was not
responding. At the time that this test was performed, some of our R\&D engineers
were already running simulations with meshes of around 200 Millions cells and,
recently (in June 2014) with 400 Millions cells. This implies that copying the
results from the computer to their scientific stations is not possible, first
because of the long transfer time and second because what the Table XXX shows:
they will be block in their visualisation or other post-processing tasks. Then,
should EDF engineers that run 400 Millions cells simulations visualise a 10
Millions lower resolution version to analyse his/her results using SALOME on a
PC? If this is the case, the visualisation of the results appears as a serious
bottleneck for our industrial system. This motivated the beginning of this work.

\begin{table}
\centering
\begin{tabular}{|p{1.5cm}|p{2.0cm}|p{2.70cm}|p{1.50cm}|p{2cm}|p{2cm}|}
\hline
\multicolumn{6}{|c|}{\textbf{MESH SIZE MANIPULATION EXPERIMENT}}\\
\hline
Number of cells & 10 Thousands & 100 Thousands & 1 Millions & 10 Millions & 50 Millions \\
\hline
RAM(\%) & <50\% & <50\% & <50\% & 100\% & Saturated \\
\hline
Reaction time & Inmediate & Inmediate & 2 to 3 seconds & Unconfortable & Not responding \\
%\hline
 %& & & \\
%$CASE$\_$A$ & 51M hexahedrals, \newline industrial size case & \textbf{heavy}:
%\newline volume rendering, \newline celldatatopointdata \newline and glyphs  &
%5a 5c 5e\\
\hline
\end{tabular}
%\vspace{-0.1in}
\caption{Subjective characterization of the reaction time of the SALOME platform for different mesh sizes.}
\label{fig:tab}
%\end{figure}
\vspace{-0.15in}
\end{table}

A first solution consists in the installation of parallel visualisation servers
that can deal with the large data generated by the simulations. In general, such
a system (in our case a ParaView "pvserver") is installed in a visualisation
cluster; the system reads the data and performs the visualisation operations in
parallel, while streaming images (or small 3D models) to a client in a PC. EDF
R\&D owns a visualisation cluster as part of his HPC cluster "Ivanoe", which will
be described later. This solution has also been tested, however this chapter
only deals with our experiences concerning in-situ processing. In any case, this
kind of solution implies writing and reading large data in parallel. Even if
these operations are performed in a cluster with a fast distributed file system,
in-situ processing is superior in the sense that the large date is potentially
not generated. 

In order to get more insight, we can model the whole time taken by simulation
and visualisation tasks as an addition of individual operations. For the
traditional $a posteriori$ visualisation approach:
\[
  t_{posterior} = T_s + T_w  + T_r + T_v
\]
where $T_s$ is the simulation time, $T_w$ is the time for writing the data,
$T_r$ is the time to read the data (either in paralel or sequentially) and $T_v$
is the time to perform visalisation operations and probably write visualisation
results (like videos or graphs). For the in-situ approach:
\[
t_{in-situ} = T_s + T_{process} + T_{w-in-situ} + T_{v-in-situ}
\]
where $T_s$ is the simulation time (the same as in $t_{posterior}$),
$T_{process}$
is the time to perform the visualisation operations $in-situ$, $T_{w-in-situ}$
the time to store the already processed visualisation results and
$T_{v-in-situ}$ the time that the engineer takes to visualize the videos or
other pre-process data. Comparing this two formulas we can see that
$t_{posterior} >>> t_{in-situ}$: we first skip writing and reading large volumes
of data, $T_w  + T_r >>> T_{process} + T_{w-in-situ}$; and also the
visualisation time is reduced $T_v >>> T_{v-in-situ}$ because, in the $a
posteriori$ approach, visualising means performing operations on large data
while in the $in-situ$ approach just lightweight data is involved.  In the rest
of the chapter these times will be exemplified, for instance, in the top two
images of figure 5555 the reader can compare $T_w$ and $T_s + T_w$ for different
simulations and the relationship $T_w  + T_r >>> T_{process} + T_{w-in-situ}$
becomes clear. These two images also exemplify how much I/O times grown,
relative to solver times, which is at the origin of why we need $in-situ$
techniques.

In conclusion, the whole process of "simulation + visualisation" is faster when
performed $in-situ$, furthermore the volume of the produced data in much
smaller. This is the reason that motivated this work.




