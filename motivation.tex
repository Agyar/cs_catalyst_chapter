\section{Motivation}
\label{sec:motivation}

Most numerical simulation engineers at EDF R\&D are currently visualizing lower
temporal and spatial resolution versions of their simulations, in order to avoid
I/O issues and cumbersome visualisation procedures, when large quantities of
data are involved. We believe that other industries dealing with large
simulations are having the same problem. This is the reason why we decided to
leverage the power of co-processing and $in$-$situ$ visualization. Our aim is to
provide our research-oriented engineers with an operational tool in a mid-term
basis. Thus, we have evaluated Catalyst as an industrial tool for performing
$in$-$situ$ visualization. 

First of all, it is important to better describe the scope of our industrial
visualisation solutions to understand why $in$-$situ$ processing is needed. In
Table~\ref{fig:tabmot} we show the results of a simple subjective experiment 
conducted by one of our engineers. At the end of 2012, she meshed 
a simple cube at different resolutions and then tried to visualise the results 
giving a subjective evaluation of how she could work. She used  
our open-source numerical simulation platform, SALOME,
and a standard scientific PC with 8Gb of RAM. Table~\ref{fig:tabmot} presents
the results of her subjective experiment. The study clearly shows that she
started working without an immediate system response for meshes which contain
more that 10 millions cells and for 50 million cells the system was not
responding at all. At the time that this test was performed, some of our R\&D engineers
were already running simulations with meshes containing around 200 millions cells and, 
in June 2014, with meshes reaching 400 millions cells. This implies that copying the
simulation results from the simulation cluster to the scientific stations is not practical, first
because of the long transfer time and second because, as table~\ref{fig:tabmot} shows,
the visualization and post-processing tasks cannot even run. 
It clearly appears that the visualization and post-processing of large meshes is
a serious bottleneck in this industrial context. This motivated the beginning of this work.

\begin{table}
\centering
\begin{tabular}{|p{1.5cm}|p{2.0cm}|p{2.20cm}|p{1.50cm}|p{2cm}|p{2cm}|}
\hline
\multicolumn{6}{|c|}{\textbf{MESH SIZE MANIPULATION EXPERIMENT}}\\
\hline
Number of cells & 10 Thousands & 100 Thousands & 1 Millions & 10 Millions & 50 Millions \\
\hline
RAM(\%) & <50\% & <50\% & <50\% & 100\% & Saturated \\
\hline
Reaction time & Immediate & Immediate & 2 to 3 seconds & Uncomfortable & Not responding \\
%\hline
 %& & & \\
%$CASE$\_$A$ & 51M hexahedrals, \newline industrial size case & \textbf{heavy}:
%\newline volume rendering, \newline celldatatopointdata \newline and glyphs  &
%5a 5c 5e\\
\hline
\end{tabular}
%\vspace{-0.1in}
\caption{Subjective characterization of the reaction time of the SALOME platform for different mesh sizes.}
\label{fig:tabmot}
%\end{figure}
\vspace{-0.15in}
\end{table}

A first solution to the post-processing bottleneck consists in the installation 
of parallel visualisation servers that can deal with the large amount of data generated 
by the numerical simulations. In general, such
a system (in our case a ParaView "pvserver") is installed on a visualisation
cluster; the system reads the data from disk and performs the visualisation operations in
parallel, while streaming images (or small 3D models) to a remote client hosted on a standard PC. 
EDF R\&D owns a visualisation cluster as part of its HPC cluster ``Ivanoe'', which will
be described later in this chapter. This type of solution implies writing and 
reading large data in parallel. Even if
these operations are performed on a cluster with a fast distributed file system,
$in$-$situ$ processing provides much better performances as large resulting datasets 
are potentially never generated. 

In order to get more insight, we can model the whole time taken by simulation
and visualisation tasks as an addition of individual operations. For the
traditional $a posteriori$ visualisation approach:
\begin{equation}
  t_{posterior} = T_s + T_w  + T_r + T_v
  \label{eq:first}
\end{equation}
where $T_s$ is the simulation time, $T_w$ is the time for writing the data,
$T_r$ is the time to read the data (either in parallel or sequentially) and $T_v$
is the time to perform visualization operations and probably write visualisation
results (like videos, images or graphs). For the $in$-$situ$ approach:
\begin{equation}
t_{in-situ} = T_s + T_{process} + T_{w-in-situ} + T_{v-in-situ}
  \label{eq:second}
\end{equation}
where $T_s$ is the simulation time (the same as in $t_{posterior}$),
$T_{process}$
is the time to perform the visualisation operations $in$-$situ$, $T_{w-in-situ}$
the time to store the already processed visualisation results and
$T_{v-in-situ}$ the time that the engineer takes to visualize the videos or
other pre-process data. Comparing this two formulas we can see that
$t_{posterior} >>> t_{in-situ}$ as, in the case of $in$-$situ$, we skip writing and reading large volumes
of data, $T_w  + T_r >>> T_{process} + T_{w-in-situ}$; but also the
visualisation time is reduced $T_v >>> T_{v-in-situ}$ because, in the $a
posteriori$ approach, visualising means performing operations on large data
while in the $in-situ$ approach only lightweight data is involved. In the rest
of this chapter these times will be exemplified. For instance, in the top two
images of figure~\ref{fig:animals} one can compare $T_w$ and $T_s + T_w$ for different
simulations and the relationship $T_w  + T_r >>> T_{process} + T_{w-in-situ}$
becomes clear. These two images demonstrate how quickly I/O times widen,
relative to solver times, which is why $in$-$situ$ techniques are needed.

In conclusion, the whole process of ``simulation + visualisation'' is faster when
performed $in$-$situ$, furthermore the volume of the produced data in much
smaller. This is the reason that motivated this work.




